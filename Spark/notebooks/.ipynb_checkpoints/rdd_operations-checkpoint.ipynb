{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4355407b-5277-4a9f-ad91-ee7e12033e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "\n",
    "# Create a SparkContext instance\n",
    "sc = SparkContext(\"local\", \"ExampleRDDApp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "44abb9b6-9df1-4c73-9445-f41d20757d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example RDD\n",
    "\n",
    "data = [1, 2, 3, 4, 5]\n",
    "rdd = sc.parallelize(data)\n",
    "\n",
    "sentences = [\"Hello World\", \"Apache Spark\", \"RDD Transformations Wide Vs Narrow in Spark\"]\n",
    "sentences_rdd = sc.parallelize(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cf83b02-e13b-4f5c-9619-be1176a64e33",
   "metadata": {},
   "source": [
    "# Transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00ec3754-ac30-49a0-8054-06192d3d05c0",
   "metadata": {},
   "source": [
    "## `Map` Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d9798f03-5653-4211-80b1-81c72d63de04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### 1. map ###\n",
      "Description: Return a new RDD by applying a function to all elements of this RDD.\n"
     ]
    }
   ],
   "source": [
    "# 1. map\n",
    "print(\"### 1. map ###\")\n",
    "print(\"Description: Return a new RDD by applying a function to all elements of this RDD.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c7c41e90-7dfc-47b8-a616-bb8617448484",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "01 map example (multiply by 2): [2, 4, 6, 8, 10]\n"
     ]
    }
   ],
   "source": [
    "# Example 1: Multiply each element by 2\n",
    "simple_map = rdd.map(lambda x: x * 2).collect()\n",
    "print(f\"01 map example (multiply by 2): {simple_map}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4d0413d3-9b8c-4e5c-bbaa-699acdc69794",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "02 word length map example (word count in sentences): [2, 2, 7]\n"
     ]
    }
   ],
   "source": [
    "# Example 2: Extract the length of each word in a list of sentences\n",
    "word_map = sentences_rdd.map(lambda sentence: len(sentence.split(\" \"))).collect()\n",
    "print(f\"02 word length map example (word count in sentences): {word_map}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbccfc21-1a0d-42be-8025-854d35b8a6d7",
   "metadata": {},
   "source": [
    "## `Filter` Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5d6f5365-bb0b-4c83-b122-ac591d00d167",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### 2. filter ###\n",
      "Description: Return a new RDD containing only the elements that satisfy a predicate.\n"
     ]
    }
   ],
   "source": [
    "# 2. filter\n",
    "print(\"### 2. filter ###\")\n",
    "print(\"Description: Return a new RDD containing only the elements that satisfy a predicate.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2a0df60d-6afb-4e0d-8ec1-8747febef1fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "01 filter example (even numbers): [2, 4]\n"
     ]
    }
   ],
   "source": [
    "# 01 Example: Filter out the even numbers\n",
    "simple_filter = rdd.filter(lambda x: x % 2 == 0).collect()\n",
    "print(f\"01 filter example (even numbers): {simple_filter}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d6c33728-1257-4534-8bb0-e949c5451ecb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "02 word example (sentences with 'Spark'): ['Apache Spark', 'RDD Transformations Wide Vs Narrow in Spark']\n"
     ]
    }
   ],
   "source": [
    "# 02 Example: Filter sentences containing the word 'Spark'\n",
    "words_filter = sentences_rdd.filter(lambda sentence: \"Spark\" in sentence).collect()\n",
    "print(f\"02 word example (sentences with 'Spark'): {words_filter}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84f06d9b-fd20-4852-b12f-697e39ad28ff",
   "metadata": {},
   "source": [
    "## `FlatMap` Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2b762095-071e-43f7-a03b-ed34fd7c4254",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### 3. flatMap ###\n",
      "Description: Return a new RDD by applying a function to all elements of this RDD and then flattening the results.\n"
     ]
    }
   ],
   "source": [
    "# 3. flatMap\n",
    "print(\"### 3. flatMap ###\")\n",
    "print(\"Description: Return a new RDD by applying a function to all elements of this RDD and then flattening the results.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "30c88faf-883a-41e3-bc4b-cef19f610a7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "01 sentences mapped: [['Hello', 'World'], ['Apache', 'Spark'], ['RDD', 'Transformations', 'Wide', 'Vs', 'Narrow', 'in', 'Spark']]\n",
      "02 flatMap example (split sentences into words): ['Hello', 'World', 'Apache', 'Spark', 'RDD', 'Transformations', 'Wide', 'Vs', 'Narrow', 'in', 'Spark']\n"
     ]
    }
   ],
   "source": [
    "# 01 Example: Split sentences into words\n",
    "sentences_mapped = sentences_rdd.map(lambda sentence: sentence.split(\" \")).collect()\n",
    "print(f\"01 sentences mapped: {sentences_mapped}\")\n",
    "\n",
    "simple_flatMap = sentences_rdd.flatMap(lambda sentence: sentence.split(\" \")).collect()\n",
    "print(f\"02 flatMap example (split sentences into words): {simple_flatMap}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a65b0c4b-c93c-403e-8ee6-55dfe8b19640",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "flatten_list  flatMap example (flatten list of lists): [1, 2, 3, 4, 5, 6, 7, 8, 9]\n"
     ]
    }
   ],
   "source": [
    "# example_Example: Flatten a list of lists\n",
    "nested_lists = [[1, 2, 3], [4, 5], [6, 7, 8, 9]]\n",
    "nested_rdd = sc.parallelize(nested_lists)\n",
    "flatten_list = nested_rdd.flatMap(lambda x: x).collect()\n",
    "print(\"flatten_list  flatMap example (flatten list of lists):\", flatten_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0864e64-1348-43f6-9277-bc4d6f4cef48",
   "metadata": {},
   "source": [
    "## `Reduce` Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1047789b-41ff-439e-a73d-cdc03e8d1729",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "#####  4. reduce ###\n",
      "Description: Reduces the elements of this RDD using the specified commutative and associative binary operator.\n"
     ]
    }
   ],
   "source": [
    "# 4. reduce\n",
    "print(\"\\n#####  4. reduce ###\")\n",
    "print(\"Description: Reduces the elements of this RDD using the specified commutative and associative binary operator.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "94be3116-a96f-45be-ae9b-c7716f174d17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "01 reduce example (sum of elements): 15\n"
     ]
    }
   ],
   "source": [
    "# 01 Example: Sum of elements\n",
    "simple_reduce = rdd.reduce(lambda x, y: x + y)\n",
    "print(\"01 reduce example (sum of elements):\", simple_reduce)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1ba93b61-92ab-4ea3-85b2-d3d984877ca5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reduce example (longest word): hippopotamus\n"
     ]
    }
   ],
   "source": [
    "# example_Example: Find the longest word in a list of words\n",
    "words = [\"cat\", \"elephant\", \"rat\", \"hippopotamus\"]\n",
    "words_rdd = sc.parallelize(words)\n",
    "words_rdd_reduced = words_rdd.reduce(lambda x, y: x if len(x) > len(y) else y)\n",
    "print(\"reduce example (longest word):\", words_rdd_reduced)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "198404ef-0d9a-45c6-83cf-1ca18dc677b9",
   "metadata": {},
   "source": [
    "## `groupByKey` Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f981c89d-c67a-47b3-8292-46fe3aa00464",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "#####  5. groupByKey ###\n",
      "Description: Group the values for each key in the RDD into a single sequence.\n"
     ]
    }
   ],
   "source": [
    "# 5. groupByKey\n",
    "print(\"\\n#####  5. groupByKey ###\")\n",
    "print(\"Description: Group the values for each key in the RDD into a single sequence.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7143026b-bea1-49a1-b712-94885e153d29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "01 groupByKey example (group numbers): [(1, ['a', 'ali']), (2, ['b']), (3, ['c']), (4, ['d']), (5, ['e'])]\n"
     ]
    }
   ],
   "source": [
    "# 01 Example: Group numbers by even and odd\n",
    "pairs = [(1, 'a'),(1, 'ali'), (2, 'b'), (3, 'c'), (4, 'd'), (5, 'e')]\n",
    "pairs_rdd = sc.parallelize(pairs)\n",
    "simple_groupByKey = pairs_rdd.groupByKey().mapValues(list).collect()\n",
    "print(\"01 groupByKey example (group numbers):\", simple_groupByKey)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "eb3f1a4d-1e82-4407-b6d2-2c8456f873c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "words_grouped example (group words by starting letter): [('cat', [1]), ('car', [2]), ('dog', [3]), ('deer', [4]), ('elephant', [5, 20])]\n"
     ]
    }
   ],
   "source": [
    "# example_Example: Group words by their starting letter\n",
    "words_pairs = [(\"cat\", 1), (\"car\", 2), (\"dog\", 3), (\"deer\", 4), (\"elephant\", 5),(\"elephant\", 20)]\n",
    "words_rdd = sc.parallelize(words_pairs)\n",
    "# mapValues(list) converts the grouped values (which are iterable) into lists.\n",
    "words_grouped = words_rdd.groupByKey().mapValues(list).collect()\n",
    "print(\"words_grouped example (group words by starting letter):\", words_grouped)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "831ea60d-cacd-467d-b70d-b4759ad80964",
   "metadata": {},
   "source": [
    "## `reduceByKey` Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8f3ae448-80c6-48b0-8835-b8427c825d68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "#####  6. reduceByKey ###\n",
      "Description: Merge the values for each key using an associative and commutative reduce function.\n"
     ]
    }
   ],
   "source": [
    "# 6. reduceByKey\n",
    "print(\"\\n#####  6. reduceByKey ###\")\n",
    "print(\"Description: Merge the values for each key using an associative and commutative reduce function.\")\n",
    "\n",
    "pairs = [(1, 'a'),(1, '_a'), (2, 'b'), (2, '_b'), (3, 'c'), (4, 'd'), (5, 'e')]\n",
    "pairs_rdd = sc.parallelize(pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f0f2abfa-6169-4b29-ad9a-b577ef103e5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "01 reduceByKey example (sum values by key): [(1, 'a_a'), (2, 'b_b'), (3, 'c'), (4, 'd'), (5, 'e')]\n"
     ]
    }
   ],
   "source": [
    "# 01 Example: Sum values with the same key\n",
    "simple_reduceByKey = pairs_rdd.reduceByKey(lambda x, y: x + y).collect()\n",
    "print(\"01 reduceByKey example (sum values by key):\", simple_reduceByKey)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "940333ce-6220-4036-bb03-abc919d590bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "example_ reduceByKey example (word count): [('cat', 2), ('dog', 3), ('elephant', 1)]\n"
     ]
    }
   ],
   "source": [
    "# example_Example: Count the occurrences of each word in a list\n",
    "word_list = [\"cat\", \"cat\", \"dog\", \"elephant\", \"dog\", \"dog\"]\n",
    "word_pairs_rdd = sc.parallelize(word_list).map(lambda word: (word, 1))\n",
    "example__reduceByKey = word_pairs_rdd.reduceByKey(lambda x, y: x + y).collect()\n",
    "print(\"example_ reduceByKey example (word count):\", example__reduceByKey)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d178084-1950-4c92-a23f-090ee1780b2e",
   "metadata": {},
   "source": [
    "## `join` Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "236a0f72-8568-40ed-b7a8-a2b2bdcbfe9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "#####  7. join ###\n",
      "Description: Perform an inner join of this RDD and another one.\n"
     ]
    }
   ],
   "source": [
    "# 7. join\n",
    "print(\"\\n#####  7. join ###\")\n",
    "print(\"Description: Perform an inner join of this RDD and another one.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5984e348-d14f-4984-a194-546f58e2283d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "01 join fruits_color_join (join two RDDs): [(2, ('banana', 'yellow')), (1, ('apple', 'red'))]\n"
     ]
    }
   ],
   "source": [
    "# 01 Example: Join two RDDs by key\n",
    "fruits = sc.parallelize([(1, \"apple\"), (2, \"banana\")])\n",
    "colors = sc.parallelize([(1, \"red\"), (2, \"yellow\")])\n",
    "fruits_color_join = fruits.join(colors).collect()\n",
    "print(\"01 join fruits_color_join (join two RDDs):\", fruits_color_join)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "79ca1159-9e12-41df-961a-d3e6d4f53750",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "join example (employee-department join): [(2, ('Jane', 'Finance')), (1, ('John', 'HR'))]\n"
     ]
    }
   ],
   "source": [
    "# example_Example: Join employee data with department data\n",
    "employees = sc.parallelize([(1, \"John\"), (2, \"Jane\"), (3, \"Joe\")])\n",
    "departments = sc.parallelize([(1, \"HR\"), (2, \"Finance\")])\n",
    "employees_department_join = employees.join(departments).collect()\n",
    "print(\"join example (employee-department join):\", employees_department_join)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d55d45e6-c3fa-4dca-969e-2e66aa7a81d5",
   "metadata": {},
   "source": [
    "## `cogroup` Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5ccfc06f-4c22-4e1c-b993-a4427c39bc56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "#####  8. cogroup ###\n",
      "Description: Group data from two RDDs sharing the same key.\n"
     ]
    }
   ],
   "source": [
    "# 8. cogroup\n",
    "# The cogroup function in PySpark is used to group data from two RDDs that share the same key. \n",
    "# It combines the values of matching keys from both RDDs into a tuple of lists.\n",
    "print(\"\\n#####  8. cogroup ###\")\n",
    "print(\"Description: Group data from two RDDs sharing the same key.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d771dbf4-0785-4159-ac6a-f6a99b3b1144",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "01 cogroup example (group two RDDs): [(2, (['banana'], ['yellow'])), (1, (['apple'], ['red'])), (3, (['orange'], []))]\n"
     ]
    }
   ],
   "source": [
    "# 01 Example: Cogroup two RDDs\n",
    "fruits_rdd = sc.parallelize([(1, \"apple\"), (2, \"banana\"), (3, \"orange\")])\n",
    "colors_rdd = sc.parallelize([(1, \"red\"), (2, \"yellow\")])\n",
    "cogrouped_fruits_colors = fruits_rdd.cogroup(colors_rdd).mapValues(lambda x: (list(x[0]), list(x[1]))).collect()\n",
    "print(\"01 cogroup example (group two RDDs):\", cogrouped_fruits_colors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f67eef7e-012e-47bb-b9a2-d14458aca810",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "example_cogroup example (sales-targets cogroup): [('store1', ([100], [150])), ('store2', ([200], [])), ('store3', ([], [250]))]\n"
     ]
    }
   ],
   "source": [
    "# example_Example: Cogroup sales data with target data\n",
    "sales_rdd = sc.parallelize([(\"store1\", 100), (\"store2\", 200)])\n",
    "targets_rdd = sc.parallelize([(\"store1\", 150), (\"store3\", 250)])\n",
    "cogrouped_sales_targets = sales_rdd.cogroup(targets_rdd).mapValues(lambda x: (list(x[0]), list(x[1]))).collect()\n",
    "print(\"example_cogroup example (sales-targets cogroup):\", cogrouped_sales_targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd8da979-3cbb-4cea-b7be-8bdd0a1345f9",
   "metadata": {},
   "source": [
    "## `distinct` Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "60e48e40-b281-4531-bbed-2214f875ddb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "#####  9. distinct ###\n",
      "Description: Return a new RDD containing the distinct elements in this RDD.\n"
     ]
    }
   ],
   "source": [
    "# 9. distinct\n",
    "print(\"\\n#####  9. distinct ###\")\n",
    "print(\"Description: Return a new RDD containing the distinct elements in this RDD.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "868c7147-0e2b-46fc-a5c2-ae28ac239d96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "example_distinct example (unique words): ['cat', 'dog', 'elephant']\n"
     ]
    }
   ],
   "source": [
    "# example_Example: Unique words from a list of words\n",
    "words = [\"cat\", \"dog\", \"cat\", \"elephant\", \"dog\"]\n",
    "words_rdd = sc.parallelize(words)\n",
    "example__distinct = words_rdd.distinct().collect()\n",
    "print(\"example_distinct example (unique words):\", example__distinct)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d02ab15-44c1-4429-a208-72b186a14908",
   "metadata": {},
   "source": [
    "# `repartition` Vs. `coalesce` Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "deb5ad55-946b-48da-91d5-10e3325c09fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, expr\n",
    "from pyspark.sql import SparkSession\n",
    "import random\n",
    "import string\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Create SparkSession\n",
    "spark = SparkSession.builder.master(\"local\").appName(\"ExampleRepartitionApp\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f49ee062-5a01-45f8-a58a-f44f962c0e97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate random log entry\n",
    "def generate_log_entry():\n",
    "  user_id = ''.join(random.choices(string.ascii_lowercase + string.digits, k=8))\n",
    "  action = random.choice([\"login\", \"logout\", \"purchase\", \"click\", \"view\"])\n",
    "  item_id = ''.join(random.choices(string.ascii_uppercase + string.digits, k=5))\n",
    "  timestamp = (datetime.now() - timedelta(seconds=random.randint(0, 2592000))).strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "  return (user_id, action, item_id, timestamp)\n",
    "\n",
    "# Generate synthetic data\n",
    "log_entries = [generate_log_entry() for _ in range(1000000)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2bbe8633-fc7a-45ad-85d0-f9b83a6ab76e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+-------+-------------------+\n",
      "|user_id |action  |item_id|timestamp          |\n",
      "+--------+--------+-------+-------------------+\n",
      "|fc7whgyp|click   |WLBUP  |2025-02-24 03:26:10|\n",
      "|y469l9hj|click   |ZA3NH  |2025-02-17 23:10:13|\n",
      "|84fnr2n7|logout  |ISN1Z  |2025-03-04 21:02:32|\n",
      "|7ar2tfru|purchase|5W457  |2025-02-25 02:59:44|\n",
      "|c206wjye|click   |N8UFV  |2025-03-04 02:18:55|\n",
      "|7ts0eeoz|purchase|DLBPQ  |2025-02-21 04:53:16|\n",
      "|ohk520m1|logout  |U86YZ  |2025-03-09 17:38:08|\n",
      "|z3gk6mty|view    |VKY9M  |2025-03-01 17:35:08|\n",
      "|6nfe46hf|login   |BOZ8L  |2025-03-03 17:42:15|\n",
      "|uxv1jeoh|click   |XVV9K  |2025-03-04 14:29:58|\n",
      "+--------+--------+-------+-------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create DataFrame\n",
    "columns = [\"user_id\", \"action\", \"item_id\", \"timestamp\"]\n",
    "log_df = spark.createDataFrame(log_entries, columns)\n",
    "\n",
    "# Show sample data\n",
    "log_df.show(10, truncate=False)\n",
    "\n",
    "# Save to a CSV file in the DBFS (Databricks File System)\n",
    "log_df.write.csv(\"/tmp/user_logs\", header=True, mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c03bb2fd-37be-4e35-93dc-e92b7ecf6d6a",
   "metadata": {},
   "source": [
    "## `repartition` Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9eb0c18e-0730-48ca-a740-09d75a951e5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "### 10. repartition ###\n",
      "Description: Return a new RDD that has exactly numPartitions partitions.\n"
     ]
    }
   ],
   "source": [
    "# 10. repartition\n",
    "#https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/rdd/RDD.scala#L480\n",
    "print(\"\\n### 10. repartition ###\")\n",
    "print(\"Description: Return a new RDD that has exactly numPartitions partitions.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f34ee678-71de-407a-903b-f9accacfdab5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Partitions: 2\n"
     ]
    }
   ],
   "source": [
    "logs_rdd = sc.textFile(\"/tmp/user_logs\")\n",
    "\n",
    "# Initial number of partitions\n",
    "initial_partitions = logs_rdd.getNumPartitions()\n",
    "print(f\"Initial Partitions: {initial_partitions}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9426cc4f-54eb-46fc-9455-18720312623e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New Partitions after Repartition: 100\n"
     ]
    }
   ],
   "source": [
    "# Repartition to 100 partitions\n",
    "repartitioned_rdd = logs_rdd.repartition(100)\n",
    "new_partitions = repartitioned_rdd.getNumPartitions()\n",
    "print(f\"New Partitions after Repartition: {new_partitions}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c47057f-acea-4e5d-b827-61e4701e7662",
   "metadata": {},
   "source": [
    "## `coalesce` Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "fe99cd52-bf41-4c4f-9809-587a8565d094",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "### 11. coalesce ###\n",
      "Description: Return a new RDD that is reduced into numPartitions partitions.\n"
     ]
    }
   ],
   "source": [
    "# 11. coalesce\n",
    "print(\"\\n### 11. coalesce ###\")\n",
    "print(\"Description: Return a new RDD that is reduced into numPartitions partitions.\")\n",
    "#https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/rdd/RDD.scala#L506\n",
    "#https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/rdd/CoalescedRDD.scala"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6cea2ffb-9753-4293-bd2d-6a5a8e9f6e8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Partitions: 2\n"
     ]
    }
   ],
   "source": [
    "# Initial number of partitions\n",
    "initial_partitions = logs_rdd.getNumPartitions()\n",
    "print(f\"Initial Partitions: {initial_partitions}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "790e4861-3c9c-434f-b5bc-520f9b9c277b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new_partitions_1 after Coalesce: 1\n"
     ]
    }
   ],
   "source": [
    "# Coalesce to 4 partitions\n",
    "coalesced_rdd_1 = logs_rdd.coalesce(1)\n",
    "new_partitions_1 = coalesced_rdd_1.getNumPartitions()\n",
    "print(f\"new_partitions_1 after Coalesce: {new_partitions_1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "aed86e43-494a-4e78-a390-220e7334fbaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new_partitions_2 after Coalesce: 2\n"
     ]
    }
   ],
   "source": [
    "# Coalesce to 50 partitions\n",
    "coalesced_rdd_2 = logs_rdd.coalesce(10)\n",
    "new_partitions_2 = coalesced_rdd_2.getNumPartitions()\n",
    "print(f\"new_partitions_2 after Coalesce: {new_partitions_2}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
